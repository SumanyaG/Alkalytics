\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{array}
\usepackage{caption}
\usepackage{amssymb}
\usepackage{float}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}
\usepackage{longtable}


\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Date 1 & 1.0 & Notes\\
Date 2 & 1.1 & Notes\\
\bottomrule
\end{tabularx}

~\\
\wss{The intention of the VnV plan is to increase confidence in the software.
However, this does not mean listing every verification and validation technique
that has ever been devised.  The VnV plan should also be a \textbf{feasible}
plan. Execution of the plan should be possible with the time and team available.
If the full plan cannot be completed during the time available, it can either be
modified to ``fake it'', or a better solution is to add a section describing
what work has been completed and what work is still planned for the future.}

\wss{The VnV plan is typically started after the requirements stage, but before
the design stage.  This means that the sections related to unit testing cannot
initially be completed.  The sections will be filled in after the design stage
is complete.  the final version of the VnV plan should have all sections filled
in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
\wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  \bottomrule
\end{tabular}\\

\wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS
  \citep{SRS} tables, if appropriate}

\wss{Remove this section if it isn't needed}

\newpage

\pagenumbering{arabic}

This document ... \wss{provide an introductory blurb and roadmap of the
  Verification and Validation plan}

\section{General Information}

\subsection{Summary}

\wss{Say what software is being tested.  Give its name and a brief overview of
  its general functions.}

\subsection{Objectives}

\wss{State what is intended to be accomplished.  The objective will be around
  the qualities that are most important for your project.  You might have
  something like: ``build confidence in the software correctness,''
  ``demonstrate adequate usability.'' etc.  You won't list all of the qualities,
  just those that are most important.}

\wss{You should also list the objectives that are out of scope.  You don't have 
the resources to do everything, so what will you be leaving out.  For instance, 
if you are not going to verify the quality of usability, state this.  It is also 
worthwhile to justify why the objectives are left out.}

\wss{The objectives are important because they highlight that you are aware of 
limitations in your resources for verification and validation.  You can't do everything, 
so what are you going to prioritize?  As an example, if your system depends on an 
external library, you can explicitly state that you will assume that external library 
has already been verified by its implementation team.}

\subsection{Challenge Level and Extras}

\wss{State the challenge level (advanced, general, basic) for your project.
Your challenge level should exactly match what is included in your problem
statement.  This should be the challenge level agreed on between you and the
course instructor.  You can use a pull request to update your challenge level
(in TeamComposition.csv or Repos.csv) if your plan changes as a result of the
VnV planning exercise.}

\wss{Summarize the extras (if any) that were tackled by this project.  Extras
can include usability testing, code walkthroughs, user documentation, formal
proof, GenderMag personas, Design Thinking, etc.  Extras should have already
been approved by the course instructor as included in your problem statement.
You can use a pull request to update your extras (in TeamComposition.csv or
Repos.csv) if your plan changes as a result of the VnV planning exercise.}

\subsection{Relevant Documentation}

\wss{Reference relevant documentation.  This will definitely include your SRS
  and your other project documents (design documents, like MG, MIS, etc).  You
  can include these even before they are written, since by the time the project
  is done, they will be written.  You can create BibTeX entries for your
  documents and within those entries include a hyperlink to the documents.}

\citet{SRS}

\wss{Don't just list the other documents.  You should explain why they are relevant and 
how they relate to your VnV efforts.}

\section{Plan}
This section outlines the Verification and Validation (VnV) responsibilities of each 
team member and the team supervisor. It presents comprehensive verification plans for the 
Software Requirements Specification (SRS), design documents (including MIS, MG, and System Design),
the VnV plan itself, and the system implementation.\\
\newline Additionally, the automated testing and verification tools intended for use in the VnV process
have been specified. Verification tasks will be carried out by the VnV team in a sequential manner
as each phase progresses, with continuous verification of the SRS and design documents as source code
is developed. Software validation will be conducted in parallel with other VnV activities, 
supporting the Proof of Concept (PoC) and final demonstration phases.

\subsection{Verification and Validation Team}
In this section, Table \ref{table:vnv_team} presents the members of the VnV team designated to carry
out the tasks specified in this document. For each team member, their role in the project's verification
process is outlined, with key details highlighting their respective responsibilities.

\begin{table}[H]
  \centering
  \caption{Validation and Verification Team Members and Responsibilities}
  \label{table:vnv_team}
  \begin{tabularx}{\textwidth}{|>{\centering\arraybackslash}p{0.3\textwidth}|>{\centering\arraybackslash}X|}
      \hline
      \textbf{Team Member} & \textbf{Role and Responsibilities} \\
      \hline
      Bassel Abdelkader & Advisor and one of the primary reviewers of documentation; contributor to the
      validation of user-based testing, providing suggestions and feedback to improve software functionality
      and user experience. \\
      \hline
      Dr. Charles de Lannoy & Advisor and one of the primary reviewers of documentation; contributor to the
      validation of user-based testing, providing suggestions and feedback to improve software functionality
      and user experience. \\
      \hline
      Jason Tran & Review the work of other team members to uphold high standards, provide suggestions for
      improvement, and maintain feedback checklists for each work item, with an emphasis on backend code. \\
      \hline
      Jennifer Ye & Review the work of other team members to uphold high standards, provide suggestions for
      improvement, and maintain feedback checklists for each work item, with an emphasis on frontend code. \\
      \hline
      Kate Min & Review the work of other team members to uphold high standards, provide suggestions for
      improvement, and maintain feedback checklists for each work item, with an emphasis on unit testing. \\
      \hline
      Sumanya Gulati & Review the work of other team members to uphold high standards, provide suggestions
      for improvement, and maintain feedback checklists for each work item, with an emphasis on the documentation
      and VnV Plan. \\
      \hline
      Other Design Teams & Peer reviewers identify issues and offer feedback and suggestions for improving
      documentation. \\
      \hline
  \end{tabularx}
\end{table}

\subsection{SRS Verification Plan}

The Software Requirements Specification (SRS) is essential for defining the project scope and guiding
implementation. To ensure it aligns with project standards and objectives, a rigorous verification
plan will be implemented, incorporating iterative feedback from peers and our assigned Teaching Assistant,
Chris Schankula.\\
\newline
The verification process will begin with an initial review to evaluate the SRS for
clarity, completeness, and alignment with project goals. Feedback will be systematically gathered and
categorized to identify areas needing refinement. Exploratory assessment techniques will allow reviewers
to interact with application prototypes and design mockups, ensuring requirements are realistically
translated into application features.

\subsubsection{Functional Testing}
Functional testing based on the SRS will validate application behavior, confirming that each function
performs according to specified requirements. The feedback will be integrated into the SRS, leading to
revisions that enhance clarity and accuracy. A comprehensive final review will ensure all feedback
has been effectively addressed, solidifying the SRS as a true reflection of the intended system.

\subsubsection{Formal and Ad-hoc Reviews}
The verification approach includes formal and ad-hoc feedback methods. Formal reviews will create
updated checklists based on existing ones and grading feedback, while review meetings with
the supervisor will identify mistakes and suggest improvements. Ad-hoc peer reviews from other design
teams will also provide valuable insights. Key verification checks will ensure the adequacy,
feasibility, and verifiability of requirements, along with traceability to use cases.

\subsubsection{Checklist}
An initial verification plan checklist will be maintained and updated over time, focusing on detailed
descriptions, relevance, traceability, verifiability, and feasibility of requirements, as well as
closing all reviewer-identified issues. By leveraging diverse insights, the SRS will be refined
to facilitate a smooth transition into the development phase, laying a strong foundation for the
project's success.\\
\newline
The following checklist has been created as an initial draft that will be iteratively updated as SRS
reviews are completed over time:
\begin{itemize}
  \item[$\square$] Does each functional requirement have a clear and precise description?
  \item[$\square$] Are the rationales for each requirement clearly articulated?
  \item[$\square$] Is there a defined fit criterion for each requirement that specifies success criteria?
  \item[$\square$] Are all functional requirements complete and unambiguous?
  \item[$\square$] Are all requirements consistent with one another, avoiding conflicts?
  \item[$\square$] Are all dependencies on external systems or components identified and addressed?
\end{itemize}

\subsection{Design Verification Plan}
This section delineates the strategies and procedures the team will employ to verify the
correctness and reliability of the design of the Alkalytics application. This plan will
serve as a guideline during the testing phase to ensure that the design aligns with the
intended requirements and effectively mitigates potential hazards identified by the VnV team.

\subsubsection{Document Review}
After the completion of the initial draft of the design documentation (including the Management
Guide (MG), Management Information System (MIS), and System Design documents), each member of
the testing team will conduct a comprehensive review before submission. The objectives of this
review process are to:
\begin{itemize}
  \item Ensure that the system design aligns with all functional and non-functional requirements.
  \item Assess the accuracy of the documentation in describing the intended functionality and
  behavior of the system.
  \item Record and report any design elements that deviate from specified requirements for
  further discussion.
\end{itemize}

\subsubsection{Review Meetings}
A structured review meeting will be conducted with the supervisors once the design documents
are completed. Additionally, peer reviews from classmates will provide critical suggestions for
improvement.

\subsubsection{Code Conformity Verification}
The team will verify that the code adheres to SOLID design principles, ensuring modular and
maintainable code structures.

\subsubsection{Formal Team Review}
A formal review with team members will occur after initial document creation, allowing for
reflection on the design prior to final review. Checklists will be utilized to compare the
design documents against the Software Requirements Specification (SRS) post-verification.

\subsubsection{Checklist}
This checklist will be updated as reviews progress to ensure comprehensive verification of
the design documents:
\begin{itemize}
  \item[$\square$] Are all requirements (functional and non-functional) traceable to at least one
  implementing module in the MG?
  \item[$\square$] Have all issues raised by reviewers been addressed and resolved?
  \item[$\square$] Do all modules and components conform to the SOLID design principles?
  \item[$\square$] Are all modules assigned unambiguous tasks with well-defined inputs and outputs?
  \item[$\square$] Is the design reflective of the database structure and functionality required for
  the application?
\end{itemize}

\subsection{Verification and Validation Plan Verification Plan}
The Verification and Validation (VnV) plan, i.e., this document, must also have
a verification plan to ensure its correctness, completeness, and feasibility.
The following methods will be applied to verify the document:
\begin{itemize}
  \item \textbf{Internal document review}: All team members review each section
  to ensure quality and provide feedback/suggestions for improvement.
  \item \textbf{Peer review by classmates}: Another team reviews the contents of
  the V\&V plan and provide feedback/suggestions for improvement.
  \item \textbf{Feedback integration}: The team refines the VnV plan
  appropriately after reviewing feedback received from internal reviews, peer
  reviews, and the grading Teaching Assistant (TA).
  \item \textbf{Mutation testing}: The team performs mutation testing by
  injecting mutations (i.e., faults) into the tests to evaluate whether the
  tests can detect the mutant and verify if its behaviour is the expected
  outcome.
\end{itemize}
The following checklist serves as a guide for verifying the VnV plan:
\begin{itemize}
  \item[$\square$] Each verification plan is complete, feasible, and
  unambiguous.
  \item[$\square$] Roles and responsibilities for verification are explicitly
  and clearly defined.
  \item[$\square$] All requirements are covered by the test cases.
  \item[$\square$] Each test specifies clear inputs, expected outputs, and
  criteria for pass/fail.
  \item[$\square$] Test procedures are well-described.
  \item[$\square$] A traceability matrix is provided, mapping each test case to
  their relevant requirement(s).
\end{itemize}

\subsection{Implementation Verification Plan}
Both static and dynamic techniques will be employed to verify the implementation
for the Alkalytics project, as outlined below:
\begin{itemize}
  \item \textbf{Static Analyzers}: Linting tools will be used for static code
  analysis to identify bugs or stylistic errors, maintain code readability, and
  enforce coding standards. Specific tools to be used are listed in Section
  \ref{testingTools}.
  \item \textbf{Code Inspection/Walkthroughs}: New features or significant
  code changes will be reviewed by at least two other team members before
  merging into the main branch. The primary developer may walk the reviewers
  through the code. Code reviews will be conducted through pull requests on
  GitHub; only reviewed and approved code may be merged into the main branch.
  This process ensures code quality, readability, functionality, and allow for
  mutual understanding of all system components among the team members.
  \item \textbf{System Testing}: System testing will verify that the application
  meets both functional and non-functional requirements as specified. Details
  regarding system testing procedures and planned test cases are outlined in Section
  \ref{systemTests} of this document.
  \item \textbf{Unit Testing}: Unit testing will verify the behaviour of the
  application's individual components to ensure they perform as expected.
  The unit testing plans in Section \ref{unitTests} will be developed upon
  completion of the design specification.
\end{itemize}

\subsection{Automated Testing and Verification Tools} \label{testingTools}
Sections 10 and 11 of the
\href{https://github.com/SumanyaG/Alkalytics/blob/main/docs/DevelopmentPlan/DevelopmentPlan.pdf}{Development
Plan} discuss the tools to be used and coding standards the team will adhere to.
Below is an expansion on the referenced sections:
\begin{itemize}
  \item \textbf{Linters}: Flake8 (for Python) and ESLint (for JavaScript) are
  the linting tools the team will use, as outlined in Section 10 of the
  Development Plan. These tools will help maintain code quality by automatically
  detecting syntax errors, code inconsistencies, and potential bugs.
  \item \textbf{Unit Testing Frameworks}: The team will use Jest for front-end
  unit testing and Pytest for back-end unit testing.
  \item \textbf{Continuous Integration (CI)}: CI will be implemented using
  GitHub Actions, automating the building and testing processes throughout
  development. The team currently has a workflow that runs on all \LaTeX{}
  documentation, and additional workflows for automated testing will be added as
  development progresses. 
  \item \textbf{Code Coverage Metrics}: Code coverage will be assessed using
  the appropriate plugins/flags for the respective unit testing frameworks
  mentioned above (e.g., \texttt{pytest-cov} for Pytest, \texttt{--coverage} in Jest).
\end{itemize}

\subsection{Software Validation Plan}

\wss{If there is any external data that can be used for validation, you should
  point to it here.  If there are no plans for validation, you should state that
  here.}

\wss{You might want to use review sessions with the stakeholder to check that
the requirements document captures the right requirements.  Maybe task based
inspection?}

\wss{For those capstone teams with an external supervisor, the Rev 0 demo should 
be used as an opportunity to validate the requirements.  You should plan on 
demonstrating your project to your supervisor shortly after the scheduled Rev 0 demo.  
The feedback from your supervisor will be very useful for improving your project.}

\wss{For teams without an external supervisor, user testing can serve the same purpose 
as a Rev 0 demo for the supervisor.}

\wss{This section might reference back to the SRS verification section.}

\section{System Tests}
This section covers all tests for different areas of the system.

\subsection{Tests for Functional Requirements}

The subsections below covers each major functional of the application, from uploading data to the various outputs of post query analysis. Many of the functions come from the same user flow. By testing the subsections below guarintees that all possible user flows involving the main application functioanlities are working as expected. 

\subsubsection{Data Input and Storage}
The tests below provide a way to evaluate the correctness of data input and storagee for the following functional requirements:
\begin{itemize}
  \item FR-1
  \item FR-2
  \item FR-3
  \item FR-4
\end{itemize}

\begin{enumerate}

\item{FR-ST1}

Control: Manual
					
Initial State: Database is running and ready to intake data
					
Input: Dataset to be stored, in .CSV format
					
Output: Data that is inputted is sent into the system, then labelled and stored successfully

Test Case Derivation: When the data is sent to the system, the data should be stored somewhere and labelled properly before it can be queried.
					
How test will be performed: The test can be performed by sending a test sample of varying sizes to the storage in the system.

\end{enumerate}

\subsubsection{Data Querying and Results}
Th tests below provide a way to evaluate the data querying and visualization process of the system for the following functional requirements:
\begin{itemize}
  \item FR-5
  \item FR-6
  \item FR-7
  \item FR-8
  \item FR-9
\end{itemize}
\begin{enumerate}

  \item{FR-ST2}
  
  Control: Manual
            
  Initial State: The system is not running any jobs, and the user interface is cleared. 
            
  Input: A selection of different combinations of parameters and datasets to be queried on
            
  Output: A human-readable and customizable visualization of correct results corresponding to the selected paramters from the input
  
  Test Case Derivation: The expected output of the system is based on the query parameters selected. A user expects the data analysis to match with what they asked for, and the user is allowed to customize the visuallized data.
            
  How test will be performed: The database will be queried using multiple combinations of parameters, and the results will be compared against the expected output. The outputted visualization will then be tested for customizability.
  
  \end{enumerate}

\subsubsection{Data analysis}
    The tests below provide a way to evaluate the data analysis in the application for the following functional requirements:
    \begin{itemize}
      \item FR-10
      \item FR-11
    \end{itemize}
      \begin{enumerate}
      
        \item{FR-ST3}
        
        Control: Manual
                  
        Initial State: The application's cleared user interface which has not yet been used to query data, with no graph showing yet.
                  
        Input: A combination of parameters to query on for a selected dataset
                  
        Output: A small written human-readable paragraph explaining the input data.
       
        Test Case Derivation: To be able to understand the returned data in ways other than through a graph, a written response gives the user a variety of choices.
                 
        How test will be performed: The website interface will allow the user to pick a written analysis response. The application will look for patterns and trends in the data and will output the findings. 
        
        \end{enumerate}


      \subsubsection{Data Hygiene}
      The tests below provide a way to evaluate how the application maintains the data hygiene of the datasets related to the following functional requirements:
      \begin{itemize}
        \item FR-12
        \item FR-13
      \end{itemize}
        \begin{enumerate}
        
          \item{FR-ST4}
          
          Control: Manual
                    
          Initial State: The application's cleared user interface which has not yet been used to query data, with no graph showing yet.
                    
          Input: Dataset to be stored, in .CSV format
                    
          Output: A log file documenting errors found in the input data and/or removals of missing data.
          
          Test Case Derivation: This is to ensure the efficency of the querying and ensuring the database is only as big as it needs to be. This will also ensure that errors are dealt with by the application and are recorded to document any inconsistancies to increase traceability.
                    
          How test will be performed: After loading in a CSV file with some error in the data. A log file will be generated documenting the error after the application attempts to fix it. 
          
          \end{enumerate}

          \subsubsection{User Access}
          This tests below provide a way to evaluate how the application allows for user login related to the following functional requirements:
          \begin{itemize}
            \item FR-14
          \end{itemize}
            \begin{enumerate}
            
              \item{FR-ST5}
              
              Control: Manual
                        
              Initial State: User interface shows a login page, with no login credentials currently used
                        
              Input: Sample user credentials
                        
              Output: The page redirects to the page designated after login
              
              Test Case Derivation: The system must be able to authenticate users properly, and when authenticated, they should be given access to the application
                        
              How test will be performed: Sample credentials with different combinations of characters will be used to log in to ensure the fields handle credentials correctly.  
              
              \end{enumerate}

  \subsubsection{Data Export}
  The tests below provide a way to evaluate the export of query reports after a session for the following functional requirement:
  \begin{itemize}
    \item FR-15
  \end{itemize}
  \begin{enumerate}
    \item{FR-ST6}
    Control: Manual

    Initial State: User interface after multiple usages of data queries

    Input: User clicking the button for saving or downloading

    Output: Query report will be downloaded to the user's device

    Test Case Derivation: The user needs to be able to get a system generated report of the queries from their session, and be able to save or download that report as needed.

    How Test Will Be Performed: After making multiple queries on the data, the save/download button will be pressed to test functionality.
  \end{enumerate}


\subsection{Tests for Nonfunctional Requirements}

\wss{The nonfunctional requirements for accuracy will likely just reference the
  appropriate functional tests from above.  The test cases should mention
  reporting the relative error for these tests.  Not all projects will
  necessarily have nonfunctional requirements related to accuracy.}

\wss{For some nonfunctional tests, you won't be setting a target threshold for
passing the test, but rather describing the experiment you will do to measure
the quality for different inputs.  For instance, you could measure speed versus
the problem size.  The output of the test isn't pass/fail, but rather a summary
table or graph.}

\wss{Tests related to usability could include conducting a usability test and
  survey.  The survey will be in the Appendix.}

\wss{Static tests, review, inspections, and walkthroughs, will not follow the
format for the tests given below.}

\wss{If you introduce static tests in your plan, you need to provide details.
How will they be done?  In cases like code (or document) walkthroughs, who will
be involved? Be specific.}
\newline
Non-functional requirements will diverge from typical functional requirements
testing. Listed below are different testing methods that will be used throughout
the section.

\noindent \textbf{User Demo Assessment}: We will have users engage in a demo of the application,
and observe how they interact with the application with respect to the
requirements and criteria defined in SRS.


\subsubsection{Look and Feel requirements}
Look and feel testing is heavily subjective, and will require users to test.
These requirements will be heavily tested with the user demo assessment.

\begin{enumerate}

\item{NFR-LF1 (User Interface)\\}

NFR: LFR-1, LFR-3, LFR-4, LFR-5, LFR-6, LFR-7

Type: User Demo, Manual

Initial State: Fully functional application ready for user interaction, starting
at the login page

Input/Condition: User engagement with application 

Output/Result: Recorded observations of how the user was able to interact with
the system 

How test will be performed: 
\begin{itemize}
  \item User will be given temporary, working credentials 
  \item User will use the app, starting from the login page, and navigate
  through its functionality
  \item An observer will record how long it takes for them to figure out
  functionality and how to navigate the application
  \item Users will also be given a survey about the interface after testing

\end{itemize}
					
\item{NFR-LF2 (Device Compatibility)\\}

NFR: LFR-2

Type: Manual

Initial State: Application running on multiple different devices

Input/Condition: Manual tester's engagement with application

Output/Result: A list of inconsistencies through multiple devices

How test will be performed: 
\begin{itemize}
  \item Each member of the team will use the application across multiple
  platforms, and try to identify any inconsistencies between them
\end{itemize}
\end{enumerate}

\subsubsection{Usability and Humanity Requirements}
Usability is also a subjective area, and thus will also require the use of a
user demo assessment.
\begin{enumerate}

  \item{NFR-UH1 (User Experience)\\}
  
  NFR: UHR-1, UHR-4, UHR-5

  Type: User Demo, Manual

  Initial State: Fully functional application ready for user interaction,
  starting at the login page
  
  Input/Condition: User engagement with application
  
  Output/Result: Recorded observations of how the user was able to interact with
  the system without clicking the help button or asking for outside help*
  
  How test will be performed:
  \begin{itemize}
    \item User will be given temporary, working credentials 
    \item User will use the app, starting from the login page, and navigate
    through its functionality
    \item An observer will record how long it takes for them to figure out
    functionality and how to navigate the application
  \end{itemize}
            
  \item{NFR-UH2 (Language and Localization)\\}
  
  NFR: UHR-2
  
  Type: Manual
  
  Initial State: Fully developed application, starting at the login page
  
  Input/Condition: Manual tester using application
  
  Output/Result: A list of all identified language discrepancies
  
  How test will be performed:
  \begin{itemize}
    \item A manual tester will navigate through each web page to ensure that the
    only language being used is English (US)  
  \end{itemize}

  \item{NFR-UH3 (System Notation)\\}

  NFR: UHR-3

  Type: Manual

  Initial State: Application ready to take in data

  Input/Condition: Sample input data

  Output/Result: Error logs from unrecognized characters, or successful upload
  
  How test will be performed:
  \begin{itemize}
    \item A tester will upload a file that has scientific and mathematical symbols
    \item They will note whether or not the file was uploaded successfully, and if the data was transferred correctly, with the symbols
  \end{itemize}

  \item{NFR-UH4 (Accessibility)\\}

  NFR: UHR-6
  
  Type: Manual
  
  Initial State: Application ready for use
  
  Input/Condition: Tester engagement using third party tools
  
  Output/Result: List of accessibility issues in accordance to WCAG, and
  checklists for if page is screen-readable
  
  How test will be performed:
  \begin{itemize}
    \item A tester launch third party tools (NVDA Reader, SiteImprove)
    \item Third party tools will examine web page and provide results
    \end{itemize}
  \end{enumerate}

\subsubsection{Performance Requirements}
These requirements can be tested in ways similar to functional requirements.
Many of these requirements have thresholds defined, making testing a pass/fail
basis.
\newline \newline
Test Data Generation: Some tests will require a large, dummy set of data to be
used as input in order to test how well the system can handle heavy payloads.
Thus, large test sets of data will be automatically generated accordingly.

\begin{enumerate}
  \item{NFR-P1 (Upload Speed)\\}
  
  NFR: PR-1

  Type: Manual

  Initial State: Application navigated to upload page, ready to upload file
  
  Input/Condition: Sample .CSV files for input
  
  Output/Result: Upload duration
  
  How test will be performed:
  \begin{itemize}
    \item Different .CSV files of varying sizes will be uploaded to the system
    \item The upload functionality will include instructions for the system to
    time how long the upload took
    \item The total duration will be logged on the browser's console.

  \end{itemize}
            
  \item{NFR-P2: (Query Response Time)\\}
  
  NFR: PR-2, PR-4

  Type: Manual

  Initial State: Application navigated to querying page, ready to make query
  
  Input/Condition: Test queries
  
  Output/Result: Query response durations

  How test will be performed:
  \begin{itemize}
    \item A set of parameter/dataset combinations will be used as queries
    \item The duration of each query will be logged and compared to defined
    criteria
  \end{itemize}

  \item{NFR-P3 (Website Response Time)\\}

  NFR: PR-3

  Type: Manual

  Initial State: Application ready to use

  Input/Condition: Tester engagement

  Output/Result: Average response time of buttons on website

  How test will be performed:
  \begin{itemize}
    \item A tester will have a timer
    \item They will time the response time of different buttons, and record it
  \end{itemize}

  \item{NFR-P4 (Data Visualization Speed)\\}

  NFR: PR-5

  Type: Manual

  Initial State: Application navigated to query page, ready to make query to
  generate graphs
 
  Input/Condition: Query parameters
  
  Output/Result: Time taken to generate graphs/visualizations
  
  How test will be performed:
  \begin{itemize}
    \item A set of parameter/dataset combinations will be used as queries 
    \item A tester will manually time how long it takes for a graph to be
    generated, or the system will log it in the console
    \end{itemize}

  \item{NFR-P5 (System Accuracy)\\}

  NFR: PR-6, PR-7, PR-8

  Type: Manual

  Initial State: Application navigated to query page, ready to make query
  
  Input/Condition: Query parameters
 
  Output/Result: A check for precision of numbers in different components of
  system
  
  How test will be performed:
  \begin{itemize}
    \item A set of parameter/dataset combinations will be used as queries 
    \item A tester will manually time how long it takes for a graph to be
    generated, or the system will log it in the console
  \end{itemize}

  \item{NFR-P6 (Robustness - Backend Disruption)\\}

  NFR: PR-9
  
  Type: Manual
  
  Initial State: Application running as normal
  
  Input/Condition: Tester temporarily taking down back end
  
  Output/Result: Error message displayed
  
  How test will be performed:
  \begin{itemize}
    \item The tester will run the application as normal
    \item The tester will then disable back end services and ensure error
    messages are generated
  \end{itemize}

  \item{NFR-P7 (Robustness - Internet Disruption)\\}

  NFR: PR-10
  
  Type: Manual
  
  Initial State: Application running as normal
  
  Input/Condition: Tester temporarily disconnects internet connection

  Output/Result: Previously generated plots and previous queries still working
  
  How test will be performed:
  \begin{itemize}
    \item The tester will run the application as normal
    \item The tester will then disconnect their device from their internet connection
    \item They will then try to load previous queries and previously generated plots
  \end{itemize}

  \item{NFR-P8 (User Capacity)\\}
  
  NFR: PR-11

  Type: Manual
  
  Initial State: Three devices ready to run application
  
  Input/Condition: Multiple different queries run by the different devices
  
  Output/Result: System response time while under load
  
  How test will be performed:
  \begin{itemize}
    \item Multiple devices will make queries simultaneously
    \item Page responsiveness will be measured while system runs multiple queries
  \end{itemize}

  \item{NFR-P9 (Storage Capacity)\\}
  
  NFR: PR-12
  
  Type: Data Generation, Automated
  
  Initial State: A database with a known amount of experiment data
  
  Input/Condition: Large suite of dummy test data
  
  Output/Result: Observations on system health after large payload
  
  How test will be performed:
  \begin{itemize}
    \item Multiple, dummy sets of experiment data will be automatically generated
    \item Using a script, this data will be uploaded into the database
    \item System health will be monitored after upload is complete
  \end{itemize}
  \end{enumerate}

\subsubsection{Operational and Environmental Requirements}
  These tests are to ensure that the application works in the expected
  environment, for the expected users. They will be manual tests done by a
  person to check for an environment's compatibility with the system.

\begin{enumerate}

\item{NFR-OE1 (Operating Environment)\\}

NFR: OER-2, OER-3, OER-4

Type: Manual

Initial State: Application running on a windows device as a web application on a
Chromium based browser.

Input/Condition: Tester engagement

Output/Result: A list of all discovered issues with the application that arise
due to environment compatibility

How test will be performed: 
\begin{itemize}
  \item A tester will run the web application on their Windows device, on Google
  Chrome
  \item They will use the application as normal and try to find any issues that
  are caused due to operating environment
\end{itemize}
					
\item{NFR-OE2 (User Onboarding)\\}

NFR: OER-5

Type: Manual, User Demo

Initial State: Application running and ready for use on home screen

Input/Condition: User engagement

Output/Result: Survey results depicting subjective complexity of onboarding
process

How test will be performed: 
\begin{itemize}
  \item A user will go through the onboarding process
  \item A survey will be given after asking the user how complex they found the onboarding to be
\end{itemize}
\end{enumerate}

\subsubsection{Maintainability and Support Requirements}
These tests aim to ensure that future users of the application will not have any
issues using the application. 

\begin{enumerate}
\item{NFR-MS1 (Interface Intuitiveness)\\}

NFR: MSR-4 

Type: Manual, User Demo

Initial State: Application running, ready for use

Input/Condition: User engagement

Output/Result: Observations on user's ability to complete tasks without support

How test will be performed: 
\begin{itemize}
  \item  A pre-defined list of tasks will be created 
  \item A user will be given this set of tasks to perform on their own
  \item The user will be observed to see if they are able to perform these tasks
  without outside help from those conducting the test 
\end{itemize}
					
\item{NFR-MS2 (Cross-Browser Compatibility)\\}

NFR: MSR-6

Type: Manual

Initial State: Multiple Chromium-based web browsers open

Input/Condition: Tester Engagement

Output/Results: All abnormal behaviour of web pages observed on each different
web browser

How test will be performed: 
\begin{itemize}
  \item A tester will use the application as normal on multiple different
  browsers
  \item They will note down any issues that they find, especially if the issues
  are unique to the usage of a certain browser
\end{itemize}
\end{enumerate}

\subsection{Traceability Between Test Cases and Requirements}

\begin{longtable}{|p{0.45\linewidth}|p{0.45\linewidth}|}
  \hline
  \textbf{Req. ID} & \textbf{System Test ID} \\
  \hline
  FR-1 & FR-SLN1\\
  FR-2 & \\
  FR-3 & \\
  FR-4 & \\
  \hline
  FR-5 & FR-SLN2\\
  FR-6 & \\
  FR-7 & \\
  FR-8 & \\
  FR-9 & \\
  \hline
  FR-10 & FR-SL3\\
  FR-11 & \\
  \hline
  FR-12 & FR-SL4\\
  FR-13 & \\
  \hline
  FR-14 & FR-SL5\\
  \hline
  FR-15 & FR-SL6\\
  \hline
  UHR-1 & NFR-UH1\\
  UHR-4 & \\
  UHR-5 & \\
  \hline
  UHR-2 & NFR-UH2\\
  \hline
  UHR-3 & NFR-UH3\\
  \hline
  UHR-6 & NFR-UH4\\
  \hline
  PR-1 & NFR-P1\\
  \hline
  PR-2 & NFR-P2\\
  PR-4 & \\
  \hline
  PR-3 & NFR-P3\\
  \hline
  PR-5 & NFR-P4\\
  \hline
  PR-6 & NFR-P5\\
  PR-7 & \\
  PR-8 & \\
  \hline
  PR-9 & NFR-P6\\
  \hline
  PR-10 & NFR-P7\\
  \hline
  PR-11 & NFR-P8\\
  \hline
  PR-12 & NFR-P9\\
  \hline
  OER-2 & NFR-OE1\\
  OER-3 & \\
  OER-4 & \\
  \hline
  OER-5 & NFR-OE2\\
  \hline
  MSR-4 & NFR-MSR1\\
  \hline
  MSR-6 & NFR-MSR2\\
  \hline

\end{longtable}

\section{Unit Test Description}

\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially layout your testing strategy here.  That is, you 
can explain how tests will be selected for each module.  For instance, your test building 
approach could be test cases for each access program, including one test for normal behaviour 
and as many tests as needed for edge cases.  Rather than create the details of the input 
and output here, you could point to the unit testing code.  For this to work, you code 
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 
					
\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\item{...\\}
    
\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?}

\wss{This is a section that would be appropriate for some projects.}

\newpage{}
\section*{Appendix --- Reflection}

\wss{This section is not required for CAS 741}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable? 
  \item What pain points did you experience during this deliverable, and how
    did you resolve them?
  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage, Valgrind etc.  You should look to
  identify at least one item for each team member.
  \item For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?
\end{enumerate}

\end{document}